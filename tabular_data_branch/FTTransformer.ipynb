{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tab_transformer_pytorch import FTTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I copy-pasted the source code directly because I wanted to modify it\n",
    "# such that the FTTransformer does not use the final MLP for any classification\n",
    "# In my modified version, we get the output of the transformer during feedforward, NOT MLP output\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# feedforward and attention\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, dim * mult * 2),\n",
    "        GEGLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        print(\"to_qkv layer # output nodes:\", inner_dim)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, explainable=False):\n",
    "        h = self.heads\n",
    "        \n",
    "        print(\"x shape before self.norm(x):\", x.shape)\n",
    "        x = self.norm(x)\n",
    "        print(\"x shape after self.norm(x):\", x.shape)\n",
    "\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        print(\"q shape after self.to_qkv(x):\", q.shape)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        print(\"q shape after rearrange:\", q.shape)\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        \n",
    "        if explainable:\n",
    "            return self.to_out\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        attn_dropout,\n",
    "        ff_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout),\n",
    "                FeedForward(dim, dropout = ff_dropout),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical embedder\n",
    "\n",
    "class NumericalEmbedder(nn.Module):\n",
    "    def __init__(self, dim, num_numerical_types):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "        self.biases = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b n -> b n 1')\n",
    "        return x * self.weights + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main class\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        categories,\n",
    "        num_continuous,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head = 16,\n",
    "        dim_out = 1,\n",
    "        num_special_tokens = 2,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "        # categorical embedding\n",
    "\n",
    "        self.categorical_embeds = nn.Embedding(total_tokens, dim)\n",
    "\n",
    "        # continuous\n",
    "\n",
    "        self.numerical_embedder = NumericalEmbedder(dim, num_continuous)\n",
    "\n",
    "        # cls token\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # transformer\n",
    "\n",
    "        self.transformer = Transformer(            \n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout\n",
    "        )\n",
    "\n",
    "        # to logits\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_categ, x_numer):\n",
    "        b = x_categ.shape[0]\n",
    "\n",
    "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
    "        x_categ += self.categories_offset\n",
    "\n",
    "        x_categ = self.categorical_embeds(x_categ)\n",
    "\n",
    "        # add numerically embedded tokens\n",
    "\n",
    "        x_numer = self.numerical_embedder(x_numer)\n",
    "\n",
    "        # concat categorical and numerical\n",
    "\n",
    "        x = torch.cat((x_categ, x_numer), dim = 1)\n",
    "\n",
    "        # append cls tokens\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "\n",
    "        # attend\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # get cls token\n",
    "\n",
    "        x = x[:, 0]\n",
    "\n",
    "        # out in the paper is linear(relu(ln(cls)))\n",
    "\n",
    "#         return self.to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"matt_metadata_norm.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Body temperature</th>\n",
       "      <th>Underlying diseases</th>\n",
       "      <th>MCHC</th>\n",
       "      <th>MCH</th>\n",
       "      <th>MCV</th>\n",
       "      <th>HCT</th>\n",
       "      <th>HGB</th>\n",
       "      <th>...</th>\n",
       "      <th>FDG</th>\n",
       "      <th>LPS</th>\n",
       "      <th>U</th>\n",
       "      <th>UALB</th>\n",
       "      <th>BCF8</th>\n",
       "      <th>ASO</th>\n",
       "      <th>PS</th>\n",
       "      <th>RF</th>\n",
       "      <th>PC</th>\n",
       "      <th>LAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patient 1</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.86875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.991667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.992857</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patient 2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.040323</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.86875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.991667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.992857</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patient 3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.034946</td>\n",
       "      <td>1</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.644444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.86875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.991667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.992857</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patient 4</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.034946</td>\n",
       "      <td>1</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.86875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.991667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.992857</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patient 5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.021505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.86875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.991667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.992857</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Patient   Age  Gender  Body temperature  Underlying diseases      MCHC  \\\n",
       "0  Patient 1  81.0       0          0.983871                    1  0.736842   \n",
       "1  Patient 2  50.0       0          1.040323                    1  0.868421   \n",
       "2  Patient 3  65.0       1          1.034946                    1  0.368421   \n",
       "3  Patient 4  73.0       0          1.034946                    1  0.552632   \n",
       "4  Patient 5  64.0       1          1.021505                    1  0.342105   \n",
       "\n",
       "        MCH       MCV   HCT       HGB  ...  FDG  LPS    U  UALB     BCF8  ASO  \\\n",
       "0  0.600000  0.483333 -0.42 -0.155556  ...  0.5  0.5  0.5   0.5 -0.86875  0.5   \n",
       "1  0.642857  0.455556  0.15  0.333333  ...  0.5  0.5  0.5   0.5 -0.86875  0.5   \n",
       "2  0.585714  0.677778 -0.96 -0.644444  ...  0.5  0.5  0.5   0.5 -0.86875  0.5   \n",
       "3  0.528571  0.516667  0.09  0.177778  ...  0.5  0.5  0.5   0.5 -0.86875  0.5   \n",
       "4  0.485714  0.577778  0.15  0.133333  ...  0.5  0.5  0.5   0.5 -0.86875  0.5   \n",
       "\n",
       "         PS   RF        PC  LAC  \n",
       "0 -0.991667  0.5 -0.992857  0.5  \n",
       "1 -0.991667  0.5 -0.992857  0.5  \n",
       "2 -0.991667  0.5 -0.992857  0.5  \n",
       "3 -0.991667  0.5 -0.992857  0.5  \n",
       "4 -0.991667  0.5 -0.992857  0.5  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"Gender\", \"Underlying diseases\"]\n",
    "cont_features = [col for col in list(data_df.columns) if col not in cat_features+[\"Patient\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cat_features: 2\n",
      "Num cont_features: 125\n"
     ]
    }
   ],
   "source": [
    "print(\"Num cat_features:\", len(cat_features))\n",
    "print(\"Num cont_features:\", len(cont_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = torch.Tensor(data_df[cat_features].values).to(torch.int64)\n",
    "X_cont = torch.Tensor(data_df[cont_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cat shape: torch.Size([1521, 2])\n",
      "X_cont shape: torch.Size([1521, 125])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_cat shape:\", X_cat.shape)\n",
    "print(\"X_cont shape:\", X_cont.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_qkv layer # output nodes: 128\n"
     ]
    }
   ],
   "source": [
    "model = FTTransformer(\n",
    "    categories = (2, 2),      # Gender and Udis\n",
    "    num_continuous = len(cont_features),     # number of continuous values\n",
    "    dim = 32,                 # dimension of transformer input and output, paper set at 32\n",
    "    dim_out = 1,              # dimension of MLP output (ignored here)\n",
    "    depth = 1,                # depth, paper recommended 6\n",
    "    heads = 8,                # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,       # post-attention dropout\n",
    "    ff_dropout = 0.1          # feed forward dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape before self.norm(x): torch.Size([1521, 128, 32])\n",
      "x shape after self.norm(x): torch.Size([1521, 128, 32])\n",
      "q shape after self.to_qkv(x): torch.Size([1521, 128, 128])\n",
      "q shape after rearrange: torch.Size([1521, 8, 128, 16])\n"
     ]
    }
   ],
   "source": [
    "out = model(X_cat, X_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5913,  0.6638, -0.4280,  ..., -0.2831, -0.8897,  0.3718],\n",
       "        [-0.5915,  0.7084, -0.3349,  ..., -0.1663, -0.8588,  0.4356],\n",
       "        [-0.7374,  0.6315, -0.4598,  ...,  0.0166, -0.8086,  0.3691],\n",
       "        ...,\n",
       "        [-0.6397,  0.7095, -0.3896,  ..., -0.1789, -0.8906,  0.4230],\n",
       "        [-0.6200,  0.6721, -0.4066,  ..., -0.1645, -0.8418,  0.3589],\n",
       "        [-0.5875,  0.7461, -0.3216,  ..., -0.1899, -0.8700,  0.4087]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1521, 32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "aim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
